{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 40\n",
    "margin = 5 # time margin for arrival before appointments\n",
    "percentage_of_appointments = 0.1\n",
    "span_cost_coefficient = 20000 # adjust\n",
    "slack = 20000 # adjust\n",
    "penalty_factor = 30000000\n",
    "\n",
    "min_work_days = 7 # minimum number of work days per location\n",
    "\n",
    "work_schedule = {\n",
    "    1: [8 * 60, 17 * 60],\n",
    "    2: [7 * 60, 16 * 60],\n",
    "    3: [11 * 60, 15 * 60],\n",
    "    4: [7 * 60, 18 * 60],\n",
    "    5: [8 * 60, 17 * 60],\n",
    "    6: [8 * 60, 16 * 60],\n",
    "    7: [12 * 60, 16 * 60]\n",
    "}\n",
    "# week (mon-sun) constraints\n",
    "max_days_off = 0\n",
    "days_off = {5,6,7} # default Sunday no work\n",
    "max_overnight_stays = 2\n",
    "no_overnight_stays = {1,2}\n",
    "if len(no_overnight_stays) > 0:\n",
    "    possible_overnight_stays = set(range(1, 8)) - no_overnight_stays\n",
    "else:\n",
    "    possible_overnight_stays = set(range(1, 8))\n",
    "max_short_days = 0\n",
    "short_days = {day for day, hours in work_schedule.items() if (hours[1] - hours[0]) / 60 <= 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed day 7 from days off because it's a short day or overnight stay.\n",
      "Adjusted max days off to 2 because 2 days off were defined.\n",
      "Adjusted max short days to 2 because 2 short days were defined.\n"
     ]
    }
   ],
   "source": [
    "# Check for short days or overnight stays defined on days off\n",
    "removed_days = [day for day in list(days_off) if day in short_days]\n",
    "for day in removed_days:\n",
    "    days_off.remove(day)\n",
    "    print(f\"Removed day {day} from days off because it's a short day or overnight stay.\")\n",
    "\n",
    "# Check for more days off defined than max days off\n",
    "if len(days_off) > max_days_off:\n",
    "    max_days_off = len(days_off)\n",
    "    print(f\"Adjusted max days off to {max_days_off} because {len(days_off)} days off were defined.\")\n",
    "\n",
    "# Check for more short days defined than max short days\n",
    "if len(short_days) > max_short_days:\n",
    "    max_short_days = len(short_days)\n",
    "    print(f\"Adjusted max short days to {max_short_days} because {len(short_days)} short days were defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.routing import create_nodes_dataframe, custom_clustering, plot_refined_clusters, assign_weekdays_to_clusters, plot_ind_route, plot_all_cluster_routes, create_data_model, plot_all_nodes_with_angles\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timedelta, time\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options_df():\n",
    "    def find_lists(current_list, current_sum, max_length, target_sum):\n",
    "        if current_sum > target_sum or len(current_list) > max_length:\n",
    "            return []\n",
    "        if current_sum == target_sum and len(current_list) <= max_length:\n",
    "            return [current_list]\n",
    "        results = []\n",
    "        for i in [0, 0.5] + list(range(1, 8)):\n",
    "            adjusted_sum = current_sum + (i if i > 0.6 else 1)  # Adjust sum for 0s treated as 1\n",
    "            results.extend(find_lists(current_list + [i], adjusted_sum, max_length, target_sum))\n",
    "        return results\n",
    "\n",
    "    # Generate valid lists\n",
    "    valid_lists = find_lists([], 0, 7, 7)\n",
    "\n",
    "    # Function to repeat values in the list according to their integer value\n",
    "    def repeat_values(lst, overnight_trips=0):\n",
    "        repeated_list = []\n",
    "        for num in lst:\n",
    "            if overnight_trips == 1:\n",
    "                repeated_list.extend([num] * (num - 1) + [0] if num > 0.6 else [0.5] if num == 0.5 else [0])\n",
    "            else:\n",
    "                repeated_list.extend([num] * num if num > 0.6 else [0.5] if num == 0.5 else [0])\n",
    "        return repeated_list\n",
    "\n",
    "    # Calculate days off for each list\n",
    "    def calculate_trips(lst, trip):\n",
    "        repeated_lst = repeat_values(lst)\n",
    "        days_off = []\n",
    "        for i in range(min(len(repeated_lst), 7)):\n",
    "            if repeated_lst[i] == trip:\n",
    "                days_off.append(i + 1)  # Use 1-based indexing for days\n",
    "        return days_off\n",
    "\n",
    "    # Calculate trip days for each list\n",
    "    def calculate_overnight_trips(lst):\n",
    "        repeated_lst = repeat_values(lst, overnight_trips=1)  \n",
    "        trip_days = []\n",
    "        for i in range(min(len(repeated_lst), 7)):\n",
    "            if repeated_lst[i] > 1:\n",
    "                trip_days.append(i + 1)  # Use 1-based indexing for days\n",
    "        return trip_days\n",
    "\n",
    "    def calculate_overnight_stays(lst):\n",
    "        overnight_stays = 0\n",
    "        for i in range(min(len(lst), 7)):\n",
    "            if lst[i] > 1:\n",
    "                overnight_stays += lst[i] - 1\n",
    "        return overnight_stays\n",
    "\n",
    "    data = {\n",
    "        'gaps': valid_lists,\n",
    "        'Sum': [sum(lst) for lst in valid_lists],  # Calculating sum normally, 0s count as 0\n",
    "        'Length': [len(lst) for lst in valid_lists],\n",
    "        'n_overnight_trips': [calculate_overnight_stays(lst) for lst in valid_lists],\n",
    "        'overnight_days': [calculate_overnight_trips(lst) for lst in valid_lists],\n",
    "        'short_days': [calculate_trips(lst, 0.5) for lst in valid_lists],\n",
    "        'n_short_days': [lst.count(0.5) for lst in valid_lists],  # Counting 0s as 'short days\n",
    "        'off_days': [calculate_trips(lst, 0) for lst in valid_lists],\n",
    "        'n_days_off': [lst.count(0) for lst in valid_lists], \n",
    "    }\n",
    "\n",
    "    options_df = pd.DataFrame(data)\n",
    "\n",
    "    # Filter the DataFrame based on the constraints\n",
    "    options_df = options_df[options_df['off_days'].apply(lambda x: all(day in x for day in days_off))]\n",
    "    options_df = options_df[options_df['n_days_off'].apply(lambda x: x <= max_days_off)]\n",
    "    options_df = options_df[options_df['short_days'].apply(lambda x: all(day in x for day in short_days))]\n",
    "    options_df = options_df[options_df['n_short_days'].apply(lambda x: x <= max_short_days)]\n",
    "    options_df = options_df[options_df['overnight_days'].apply(lambda x: not any(day in x for day in no_overnight_stays))]\n",
    "    options_df = options_df[options_df['n_overnight_trips'].apply(lambda x: x <= max_overnight_stays)]\n",
    "    # add a column to df containing a dictionary counting the number of times an integer >1 appears in the list\n",
    "    options_df['blocks'] = options_df['gaps'].apply(lambda x: Counter([item for item in x if item > 0]))\n",
    "    return options_df\n",
    "\n",
    "def calculate_metric(nodes_df, global_max_dist, node_ids, cluster_id, print_ind_metrics=False):\n",
    "    if len(node_ids) > 0:\n",
    "        filtered_nodes_df = nodes_df[nodes_df['node_id'].isin(node_ids)]\n",
    "\n",
    "        num_nodes_metric = len(filtered_nodes_df) / len(nodes_df)\n",
    "        \n",
    "        priority_metric = filtered_nodes_df['priority'].nlargest(int(0.5 * len(filtered_nodes_df))).mean()\n",
    "        priority_metric = priority_metric / float(cluster_id.split('_')[0])\n",
    "\n",
    "        max_dist_to_root = filtered_nodes_df['dist_to_home'].max()\n",
    "        dist_metric = max_dist_to_root / global_max_dist\n",
    "\n",
    "        # prevent any metric from being nan\n",
    "        if np.isnan(num_nodes_metric):\n",
    "            # print(f'Problems with num_nodes_metric for {cluster_id}')\n",
    "            num_nodes_metric = 0.5\n",
    "        if np.isnan(priority_metric):\n",
    "            # print(f'Problems with priority_metric for {cluster_id}')\n",
    "            priority_metric = 0.3\n",
    "        if np.isnan(dist_metric):\n",
    "            # print(f'Problems with dist_metric for {cluster_id}')\n",
    "            dist_metric = 0.5\n",
    "\n",
    "        metric = num_nodes_metric + dist_metric / 6\n",
    "\n",
    "        if print_ind_metrics:\n",
    "            print(f'Cluster: {cluster_id}')\n",
    "            print(f\"Number of nodes metric: {num_nodes_metric}\")\n",
    "            print(f\"Priority metric: {priority_metric}\")\n",
    "            print(f\"Distance metric: {dist_metric}\")\n",
    "            print(f\"Overall metric: {metric}\")\n",
    "    else:\n",
    "        metric = 0\n",
    "\n",
    "        if print_ind_metrics:\n",
    "            print('Found a cluster wihtout nodes')\n",
    "    \n",
    "    return metric\n",
    "\n",
    "def adjust_angles(clusters, nodes_df, angle_sizes, degree_adj, global_max_dist, cluster_sizes, total_span, verbose):\n",
    "    metrics = {}\n",
    "    for cluster_id, node_ids in clusters.items():\n",
    "        metrics[cluster_id] = calculate_metric(nodes_df, global_max_dist, node_ids, cluster_id)\n",
    "\n",
    "    total_metric = sum(metrics.values())\n",
    "    # total_days = sum of each key multiplied by the value in cluster_sizes\n",
    "    total_days = sum([key * value for key, value in cluster_sizes.items()])\n",
    "    base_metric = total_metric / total_days\n",
    "    \n",
    "    target_metrics = {}\n",
    "    for cluster, metric in metrics.items():\n",
    "        size = float(cluster.split('_')[0])\n",
    "        target_metrics[cluster] = base_metric * size\n",
    "\n",
    "    new_angle_sizes = angle_sizes.copy()  # Copy existing angle sizes to modify\n",
    "    \n",
    "    deviations = {}\n",
    "    for cluster_id, metric in metrics.items():\n",
    "        size = float(cluster_id.split('_')[0])\n",
    "        soll_metric = target_metrics[cluster_id]\n",
    "        deviation = metric - soll_metric\n",
    "        deviations[cluster_id] = deviation\n",
    "        new_angle_sizes[cluster_id] -= deviation * degree_adj\n",
    "\n",
    "    # Normalize the new angles to ensure they sum to total_span\n",
    "    total_new_angles = sum(new_angle_sizes.values())\n",
    "    scale_factor = total_span / total_new_angles\n",
    "    for cluster_id in new_angle_sizes:\n",
    "        new_angle_sizes[cluster_id] *= scale_factor\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Deviations, metrics, and new angle sizes:\")\n",
    "        for cluster_id in clusters:\n",
    "            print(f\"Cluster {cluster_id} with deviation {round(deviations[cluster_id], 2)}, \"\n",
    "                  f'and initial angle size {round(angle_sizes[cluster_id], 2)}° '\n",
    "                  f\"has new angle size {round(new_angle_sizes[cluster_id], 2)}°.\")\n",
    "\n",
    "    return new_angle_sizes\n",
    "\n",
    "def custom_clustering(nodes_df, cluster_sizes, precision, home_node_id=0, verbose=False, visual=False):\n",
    "    # remove the home node from the nodes_df\n",
    "    if nodes_df.index[0] == 0:\n",
    "        nodes_df_copy = nodes_df.drop(0).copy()\n",
    "    \n",
    "    clusters = {}\n",
    "    for size, count in cluster_sizes.items():\n",
    "        for i in range(count):\n",
    "            clusters[f'{size}_day_trip_{i}'] = []\n",
    "    \n",
    "    angles = sorted(nodes_df_copy['angle_to_home'])\n",
    "    diffs = [angles[i + 1] - angles[i] for i in range(len(angles) - 1)]\n",
    "    diffs.append(360 - angles[-1] + angles[0])\n",
    "    \n",
    "    max_gap = max(diffs)\n",
    "    gap_start = angles[diffs.index(max_gap)]\n",
    "    gap_end = angles[(diffs.index(max_gap) + 1) % len(angles)]\n",
    "\n",
    "    max_gap = max(diffs)\n",
    "    total_span = 360 - max_gap\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f\"Largest gap spans from {gap_start}° to {gap_end}°, covering {max_gap}° leaving a total span of {total_span} for locations.\")\n",
    "\n",
    "    total_equivalent_degrees = sum(count * size for size, count in cluster_sizes.items())\n",
    "    base_degree = total_span / total_equivalent_degrees\n",
    "\n",
    "    angle_sizes = {}\n",
    "    for size, count in cluster_sizes.items():\n",
    "        # Calculate the angular size for each cluster of this size\n",
    "        cluster_angle_size = base_degree * size\n",
    "        for i in range(count):\n",
    "            cluster_id = f'{size}_day_trip_{i}'\n",
    "            angle_sizes[cluster_id] = cluster_angle_size\n",
    "    global_max_dist = nodes_df_copy['dist_to_home'].max()\n",
    "\n",
    "    cluster_start = gap_end\n",
    "    degree_adj = total_span / 7\n",
    "\n",
    "    # Initial assignment of nodes to clusters\n",
    "    for i in range(precision):\n",
    "        current_angle = cluster_start  # Reset the start angle for each precision iteration\n",
    "            \n",
    "        # add the home node to each cluster\n",
    "        for key in clusters.keys():\n",
    "            clusters[key] = [home_node_id]\n",
    "\n",
    "        # Assign nodes to clusters based on their angle to the home node\n",
    "        for cluster_id, size in angle_sizes.items():\n",
    "            start_angle = current_angle\n",
    "            # round up to the nearest integer\n",
    "            start_angle = int(start_angle)\n",
    "            end_angle = (current_angle + size) % 360\n",
    "            end_angle = int(np.ceil(end_angle))\n",
    "            # Ensuring all nodes are assigned, handling the wrap-around scenario more cleanly\n",
    "            if end_angle < start_angle:  # This handles the case where the segment wraps past 360 degrees\n",
    "                nodes_in_cluster = [index for index, row in nodes_df_copy.iterrows() if \n",
    "                                    (row['angle_to_home'] >= start_angle or row['angle_to_home'] < end_angle)]\n",
    "            else:  # No wrap-around, normal case\n",
    "                nodes_in_cluster = [index for index, row in nodes_df_copy.iterrows() if \n",
    "                                    (start_angle <= row['angle_to_home'] < end_angle)]\n",
    "\n",
    "            clusters[cluster_id] = [home_node_id] + nodes_in_cluster\n",
    "            current_angle = end_angle\n",
    "\n",
    "        if (i == 0 and verbose):\n",
    "            print(\"Initial clusters:\")\n",
    "            for key, value in clusters.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "\n",
    "        if i % 10 == 0 and visual:\n",
    "            plot_refined_clusters(clusters, nodes_df)\n",
    "        \n",
    "        angle_sizes = adjust_angles(clusters, nodes_df_copy, angle_sizes, degree_adj, global_max_dist, cluster_sizes, total_span, verbose)\n",
    "        degree_adj *= 0.95\n",
    "\n",
    "        sum_of_angles = sum(angle_sizes.values())\n",
    "        if verbose:\n",
    "            print(f\"Sum of angles: {sum_of_angles} vs. total span: {total_span}\")\n",
    "        \n",
    "    return clusters\n",
    "\n",
    "def fit_blocks(blocks, gaps, solution, solutions, index=0):\n",
    "    if index == len(gaps):  # Check if we've addressed all gaps\n",
    "        if not blocks:  # Ensure all blocks have been used\n",
    "            solutions.append(solution)\n",
    "        return\n",
    "    \n",
    "    # Attempt to fit each block into the current gap\n",
    "    for i, block in enumerate(blocks):\n",
    "        # ['1_day_trip_5', '1_day_trip_0', '1_day_trip_1', '1_day_trip_3', '1_day_trip_4', '1_day_trip_2']\n",
    "        size = block.split('_')[0]\n",
    "        block_size = 1 if size == '0.5' else int(size)\n",
    "        if block_size <= gaps[index]:  # Check if block can fit in the current gap\n",
    "            # Setup for recursion: remove the block and reduce the gap size\n",
    "            new_blocks = blocks[:i] + blocks[i+1:]  # Remove current block\n",
    "            new_solution = [lst[:] for lst in solution]  # Copy solution to modify\n",
    "            new_solution[index].append(block)  # Add block to current gap's solution\n",
    "            \n",
    "            # Reduce the gap by the size of the block\n",
    "            new_gaps = gaps[:]\n",
    "            new_gaps[index] -= block_size\n",
    "            \n",
    "            # Move to next gap if current gap is exactly filled, otherwise continue\n",
    "            if new_gaps[index] == 0:\n",
    "                fit_blocks(new_blocks, new_gaps, new_solution, solutions, index + 1)\n",
    "            else:\n",
    "                fit_blocks(new_blocks, new_gaps, new_solution, solutions, index)\n",
    "\n",
    "def count_fixed_appointments(weekly_schedule, nodes_df):\n",
    "    count = 0\n",
    "    for i, cluster_day in enumerate(weekly_schedule):\n",
    "        start_day = i + 1\n",
    "        if 'day_off' not in cluster_day:\n",
    "            duration = int(cluster_day.split('_')[0])\n",
    "            end_day = start_day + duration\n",
    "            for day in range(start_day, end_day + 1):\n",
    "                count += nodes_df[(nodes_df['cluster'] == cluster_day) & (nodes_df['weekdays_fixed_appointments'] == day)].shape[0]\n",
    "    return count\n",
    "\n",
    "def time_to_minutes(t):\n",
    "    return t.hour * 60 + t.minute + t.second\n",
    "\n",
    "def adjust_opening_hours(row, work_schedule):\n",
    "    clusters = row['cluster']\n",
    "    first_day = min(clusters)\n",
    "    opening_hours = row['opening_hours']\n",
    "    fixed_appointment = row['fixed_appointment']\n",
    "    on_site_time = row['on_site_time']\n",
    "    adjusted_hours = []\n",
    "    \n",
    "    if isinstance(fixed_appointment, list):\n",
    "        day, app_start, _ = fixed_appointment[0], fixed_appointment[1], fixed_appointment[2]\n",
    "        adjusted_open = time_to_minutes(app_start) + 1440 * (day - first_day) - margin \n",
    "        adjusted_close = adjusted_open\n",
    "        adjusted_hours = [[adjusted_open, adjusted_close]]\n",
    "    else:\n",
    "        for day, intervals in opening_hours.items():\n",
    "            if day in clusters:  # Ensure we only adjust days that are in clusters\n",
    "                for start, end in intervals:\n",
    "                    work_start, work_end = work_schedule[day]\n",
    "                    start = max(work_start, time_to_minutes(start))\n",
    "                    end = min(work_end, time_to_minutes(end))\n",
    "\n",
    "                    # Adjust each interval's start and end times\n",
    "                    adjusted_start = start + 1440 * (day - first_day)\n",
    "                    adjusted_end = end + 1440 * (day - first_day) - on_site_time # needs to finish before store closes or work ends\n",
    "                    adjusted_hours.append([adjusted_start, adjusted_end])\n",
    "    return adjusted_hours\n",
    "\n",
    "def minutes_to_hhmm(minutes_am, days = False):\n",
    "    if days:\n",
    "        day = minutes_am // 1440\n",
    "    minutes_am = minutes_am % 1440  # Ensure minutes are within a day\n",
    "    minutes = minutes_am % 60\n",
    "    hours = (minutes_am - minutes) // 60  # Use integer division for hours\n",
    "    if days:\n",
    "        return f'{hours:02}:{minutes:02} (day {day + 1})'\n",
    "    else:\n",
    "        return f'{hours:02}:{minutes:02}'\n",
    "\n",
    "def create_data_model(sub_nodes_df, sub_time_matrix):\n",
    "    \"\"\"Stores the data for the problem.\"\"\"\n",
    "    data = {}\n",
    "    data['time_matrix'] = sub_time_matrix\n",
    "    data['windows'] = sub_nodes_df['adjusted_opening_hours'].tolist()\n",
    "    data['priorities'] = sub_nodes_df['priority'].tolist()\n",
    "    data['num_vehicles'] = 1\n",
    "    data['on_site_time'] = sub_nodes_df['on_site_time'].tolist()\n",
    "    data['depot'] = 0\n",
    "    return data\n",
    "\n",
    "def return_route_and_times(solution, manager, routing, original_node_ids, data):\n",
    "    \"\"\"Returns the route along with the start times at each node.\"\"\"\n",
    "    index = routing.Start(0)  # Start at the depot.\n",
    "    route_with_travel = []\n",
    "    route_without_travel = []\n",
    "    time_dimension = routing.GetDimensionOrDie('total_time')  # Make sure this matches the dimension name used\n",
    "\n",
    "    while not routing.IsEnd(index):\n",
    "        node_index = manager.IndexToNode(index)\n",
    "        original_node_id = original_node_ids[node_index]  # Map back to original node ID\n",
    "        time_var = time_dimension.CumulVar(index)\n",
    "        start_time = solution.Min(time_var)\n",
    "        end_time = start_time + data['on_site_time'][node_index]  # Include on-site time\n",
    "        route_with_travel.append((original_node_id, start_time, end_time))  # Include end time for better clarity\n",
    "        route_without_travel.append((original_node_id, start_time))  # Include end time for better clarity\n",
    "        next_index = solution.Value(routing.NextVar(index))\n",
    "        \n",
    "        travel_time = routing.GetArcCostForVehicle(index, next_index, 0) - data['on_site_time'][index]  # Get travel time\n",
    "        route_with_travel.append((\"road\", travel_time))\n",
    "        \n",
    "        index = next_index\n",
    "\n",
    "    # Add the final node\n",
    "    final_node_index = manager.IndexToNode(index)\n",
    "    final_node_id = original_node_ids[final_node_index]\n",
    "    final_time_var = time_dimension.CumulVar(index)\n",
    "    final_start_time = solution.Min(final_time_var)\n",
    "    final_end_time = final_start_time + data['on_site_time'][final_node_index]\n",
    "    route_with_travel.append((final_node_id, final_start_time, final_end_time))\n",
    "    route_without_travel.append((final_node_id, final_start_time))\n",
    "\n",
    "    return route_with_travel, route_without_travel\n",
    "\n",
    "def print_route(route_with_times):\n",
    "    \"\"\"Prints the route in the desired format.\"\"\"\n",
    "    route_str = \"\"\n",
    "    for segment in route_with_times:\n",
    "        if segment[0] == \"road\":\n",
    "            route_str += f\" - road ({segment[1]}) - \"\n",
    "        else:\n",
    "            node_id, start_time, end_time = segment\n",
    "            route_str += f\"{node_id} ({minutes_to_hhmm(start_time)}-{minutes_to_hhmm(end_time)}) for a meeting of {nodes_df.loc[nodes_df['node_id'] == node_id, 'on_site_time'].iloc[0]} minutes\"\n",
    "    print(route_str)\n",
    "\n",
    "def solve_vrp(time_matrix, sub_nodes_df, deep=False, verbose=False):\n",
    "    trip_name = sub_nodes_df['cluster_name'][0]\n",
    "    trip_len = float(trip_name.split('_')[0])\n",
    "    trip = sub_nodes_df['cluster'][0]\n",
    "    first_day = min(trip)\n",
    "    last_day = max(trip)\n",
    "    \n",
    "    max_travel_time = int(10000 * trip_len)\n",
    "    \n",
    "    nodes = sub_nodes_df['node_id'].tolist()\n",
    "    \n",
    "    sub_time_matrix = time_matrix.loc[nodes, nodes].values.tolist()\n",
    "    sub_time_matrix = [[int(x) for x in row] for row in sub_time_matrix]\n",
    "    \n",
    "    data = create_data_model(sub_nodes_df, sub_time_matrix)\n",
    "    \n",
    "    manager = pywrapcp.RoutingIndexManager(len(data[\"time_matrix\"]), data[\"num_vehicles\"], data[\"depot\"])\n",
    "    routing = pywrapcp.RoutingModel(manager)\n",
    "    \n",
    "    def time_callback(from_index, to_index):\n",
    "        from_node = manager.IndexToNode(from_index)\n",
    "        to_node = manager.IndexToNode(to_index)\n",
    "        return data[\"time_matrix\"][from_node][to_node] + data['on_site_time'][from_node]\n",
    "    \n",
    "    transit_callback_index = routing.RegisterTransitCallback(time_callback)\n",
    "    \n",
    "    routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n",
    "    routing.AddDimension(\n",
    "        transit_callback_index,\n",
    "        slack,  # upper bound for slack / waiting time\n",
    "        max_travel_time,  # upper bound for vehicle maximum travel time\n",
    "        False,  # start cumul to zero\n",
    "        \"total_time\"\n",
    "    )\n",
    "    \n",
    "    time_dimension = routing.GetDimensionOrDie(\"total_time\")\n",
    "\n",
    "    # PENALTY\n",
    "    for location_index, priority in enumerate(data['priorities']):\n",
    "        index = manager.NodeToIndex(location_index)\n",
    "        if index == 0:\n",
    "            continue\n",
    "        else:\n",
    "            routing.AddDisjunction([index], int(round((priority*100)**2*penalty_factor, 0)))\n",
    "\n",
    "    # OPENING HOURS, LUNCH AND OVERNIGHT BREAKS window example:\n",
    "    for location_index, windows in enumerate(data['windows']):\n",
    "        index = manager.NodeToIndex(location_index)\n",
    "        time_dimension.CumulVar(index).SetRange(windows[0][0], windows[-1][1])\n",
    "        if verbose:\n",
    "            print(f'range for node {location_index}: {minutes_to_hhmm(windows[0][0], days = True)} - {minutes_to_hhmm(windows[-1][1], days = True)}')\n",
    "\n",
    "        ranges = [[windows[i][1], windows[i+1][0]] for i in range(len(windows) - 1)]\n",
    "        for start, end in ranges:\n",
    "            time_dimension.CumulVar(index).RemoveInterval(start, end)\n",
    "            if verbose:\n",
    "                print(f'removed interval {start} - {end} for window {windows}')\n",
    "    \n",
    "    # Agent lunch break (not sure why, if, how node_visit_transit is used or if it is even correct)\n",
    "    node_visit_transit = {}\n",
    "    for index in range(routing.Size()):\n",
    "        node = manager.IndexToNode(index)\n",
    "        node_visit_transit[index] = data['on_site_time'][node]\n",
    "    \n",
    "    lunch_breaks = []\n",
    "    for day in range(first_day, last_day + 1):\n",
    "        if trip_len >= 1:\n",
    "            lunch_duration = 29\n",
    "            lunch_start = 12 * 60 + 1440 * (day - first_day)\n",
    "            lunch_end = lunch_start + lunch_duration\n",
    "            lunch_break_interval = routing.solver().FixedDurationIntervalVar(\n",
    "                lunch_start, lunch_end, lunch_duration, False, f'lunch_break {day}'\n",
    "            )\n",
    "            lunch_breaks.append(lunch_break_interval)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'added lunch break from {lunch_start} to {lunch_end} for day {day} of {trip_name}')\n",
    "\n",
    "    time_dimension.SetBreakIntervalsOfVehicle(lunch_breaks, 0, node_visit_transit)\n",
    "\n",
    "    # Instantiate route start and end times to produce feasible times\n",
    "    routing.AddVariableMinimizedByFinalizer(time_dimension.CumulVar(routing.Start(0)))\n",
    "    routing.AddVariableMinimizedByFinalizer(time_dimension.CumulVar(routing.End(0)))\n",
    "\n",
    "    # Setting first solution heuristic\n",
    "    search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "    search_parameters.first_solution_strategy = (\n",
    "        routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n",
    "    if deep:\n",
    "        search_parameters.local_search_metaheuristic = (\n",
    "            routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH)\n",
    "    search_parameters.time_limit.seconds = 300\n",
    "    search_parameters.log_search = False\n",
    "\n",
    "    # Optional: Set a more diverse objective to avoid synchronization\n",
    "    time_dimension.SetGlobalSpanCostCoefficient(100)\n",
    "\n",
    "    # Solve the problem\n",
    "    solution = routing.SolveWithParameters(search_parameters)\n",
    "\n",
    "    if solution:\n",
    "        dropped = []\n",
    "        original_node_ids = sub_nodes_df['node_id'].tolist()\n",
    "        for node in range(routing.Size()):\n",
    "            if routing.IsStart(node) or routing.IsEnd(node):\n",
    "                continue\n",
    "            if solution.Value(routing.NextVar(node)) == node:\n",
    "                dropped.append(original_node_ids[node])\n",
    "        return return_route_and_times(solution, manager, routing, original_node_ids, data), dropped\n",
    "        \n",
    "    else:\n",
    "        print(f\"No solution found\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gaps</th>\n",
       "      <th>Sum</th>\n",
       "      <th>Length</th>\n",
       "      <th>n_overnight_trips</th>\n",
       "      <th>overnight_days</th>\n",
       "      <th>short_days</th>\n",
       "      <th>n_short_days</th>\n",
       "      <th>off_days</th>\n",
       "      <th>n_days_off</th>\n",
       "      <th>blocks</th>\n",
       "      <th>nodes_considered</th>\n",
       "      <th>num_nodes_considered</th>\n",
       "      <th>avg_prio_nodes_considered</th>\n",
       "      <th>num_nodes_dropped</th>\n",
       "      <th>total_road_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 0.5, 1, 0, 0, 0.5]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[3, 7]</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>{1: 3, 0.5: 2}</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>40</td>\n",
       "      <td>1.38</td>\n",
       "      <td>8</td>\n",
       "      <td>2114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        gaps  Sum  Length  n_overnight_trips overnight_days  \\\n",
       "0  [1, 1, 0.5, 1, 0, 0, 0.5]  4.0       7                  0             []   \n",
       "\n",
       "  short_days  n_short_days off_days  n_days_off          blocks  \\\n",
       "0     [3, 7]             2   [5, 6]           2  {1: 3, 0.5: 2}   \n",
       "\n",
       "                                    nodes_considered  num_nodes_considered  \\\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...                    40   \n",
       "\n",
       "   avg_prio_nodes_considered  num_nodes_dropped  total_road_time  \n",
       "0                       1.38                  8             2114  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual = False\n",
    "verbose = False\n",
    "\n",
    "nodes_df, time_matrix = create_nodes_dataframe(num_nodes=num_nodes, min_work_days=min_work_days, days_off=days_off, home_node_id=0, visiting_interval_min=10, visiting_interval_max=30, max_last_visit=20, frac_fixed_app=percentage_of_appointments, simple_schedule=False)\n",
    "\n",
    "options_df = get_options_df()\n",
    "options_df = options_df.sort_values(by='n_overnight_trips')\n",
    "options_df = options_df.reset_index(drop=True)\n",
    "\n",
    "if len(options_df) == 0:\n",
    "    raise ValueError(\"No valid options found.\")\n",
    "\n",
    "if verbose:\n",
    "    print('there are ', len(options_df), 'options that will be tested')\n",
    "\n",
    "options_df['nodes_considered'] = None\n",
    "options_df['num_nodes_considered'] = 0\n",
    "options_df['avg_prio_nodes_considered'] = 0.0\n",
    "options_df['num_nodes_dropped'] = 0\n",
    "options_df['total_road_time'] = 0\n",
    "\n",
    "for index, row in options_df.iterrows():\n",
    "    blocks = row['blocks']\n",
    "    gaps = row['gaps']\n",
    "    clusters = custom_clustering(nodes_df, blocks, precision=50, verbose=False, visual=False)\n",
    "\n",
    "    if verbose:\n",
    "        for cluster, nodes in clusters.items():\n",
    "            print(f\"Cluster {cluster}:\")\n",
    "            print(f\"Average node priority: {round(nodes_df.loc[nodes_df['node_id'].isin(nodes), 'priority'].mean(), 2)}\")\n",
    "            print(f\"Average distance to home: {round(nodes_df.loc[nodes_df['node_id'].isin(nodes), 'dist_to_home'].mean(), 2)}\")\n",
    "            print(f\"Count of nodes: {len(nodes)}\")\n",
    "\n",
    "    if visual:\n",
    "        plot_refined_clusters(clusters, nodes_df)\n",
    "\n",
    "    node_to_cluster = {node: cluster for cluster, nodes in clusters.items() for node in nodes}\n",
    "    nodes_df['cluster'] = nodes_df['node_id'].map(node_to_cluster)\n",
    "    nodes_df['cluster_name'] = nodes_df['cluster']\n",
    "\n",
    "    # store positions of 0s in gaps to remove and add them to the solution after fitting blocks\n",
    "    zero_positions = [i for i, gap in enumerate(gaps) if gap == 0]\n",
    "    gaps = [gap for gap in gaps if gap > 0]\n",
    "\n",
    "    short_positions = [i for i, gap in enumerate(gaps) if gap == 0.5]\n",
    "    gaps = [1 if gap == 0.5 else gap for gap in gaps]\n",
    "    \n",
    "    blocks = list(nodes_df['cluster'].unique())\n",
    "    solution = [[] for _ in gaps]  # Initialize solution structure for each gap\n",
    "    solutions = []\n",
    "    if verbose:\n",
    "        print(f'Blocks: {blocks}, Gaps: {gaps}')\n",
    "    fit_blocks(blocks, gaps, solution, solutions)\n",
    "\n",
    "    # Add 0s back to the solution as 'day-off' clusters\n",
    "    for solution in solutions:\n",
    "        for position in zero_positions:\n",
    "            solution.insert(position, ['1_day_off'])\n",
    "    solutions = [[item for sublist in outer_list for item in sublist] for outer_list in solutions]\n",
    "    repeated_list = []\n",
    "    for sublist in solutions:\n",
    "        new_sublist = []\n",
    "        for item in sublist:\n",
    "            count = item.split('_')[0]\n",
    "            count = 1 if count == '0.5' else count\n",
    "            new_sublist.extend([item] * int(count))  # Repeat the item\n",
    "        \n",
    "        repeated_list.append(new_sublist)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Found {len(repeated_list)} solutions: {repeated_list}')\n",
    "\n",
    "    best_count = -float('inf')\n",
    "    best_list = None\n",
    "    # for each list capturing the allocations of clusters to weekdays\n",
    "    for i, sublist in enumerate(repeated_list):\n",
    "        if verbose:\n",
    "            print(f'solution {i+1}/{len(repeated_list)}')\n",
    "        count = 0\n",
    "        # for each cluster in the list\n",
    "        for cluster in set(sublist):\n",
    "            if not 'day_off' in cluster:\n",
    "                # Access the indices of the cluster in the week\n",
    "                relevant_weekdays = [i + 1 for i, gap in enumerate(sublist) if gap == cluster]\n",
    "                \n",
    "                # get a list of non-unique fixed appointments\n",
    "                fixed_appointment_day = list(nodes_df[nodes_df['cluster'] == cluster]['weekday_fixed_appointment'])\n",
    "                \n",
    "                cluster_count_fixed = len([entry for entry in fixed_appointment_day if entry in relevant_weekdays])\n",
    "                initial_count = count\n",
    "                count += cluster_count_fixed\n",
    "\n",
    "                closed_days = list(nodes_df[nodes_df['cluster'] == cluster]['closed_days'].dropna())\n",
    "                flat_closed_days = []\n",
    "                for s in closed_days:\n",
    "                    flat_closed_days.extend(s)\n",
    "                \n",
    "                cluster_count_closed = len([entry for entry in flat_closed_days if entry in relevant_weekdays])\n",
    "                count -= cluster_count_closed\n",
    "                if verbose:\n",
    "                    print(f'Relevant weekdays: {relevant_weekdays}')\n",
    "                    print(f'Fixed appointment days: {fixed_appointment_day}')\n",
    "                    print(\"Closed on:\", flat_closed_days)\n",
    "                    print(f'added {cluster_count_fixed} and deducted {cluster_count_closed} from {initial_count}')\n",
    "\n",
    "        if count > best_count:\n",
    "            if verbose:\n",
    "                print(f'replacing {best_count} with {count}')\n",
    "            best_count = count\n",
    "            best_list = sublist\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Best solution had a count of {best_count}')\n",
    "\n",
    "    mapping_dict = {}\n",
    "    for i, cluster in enumerate(best_list):\n",
    "        indices = [j+1 for j, x in enumerate(best_list) if x == cluster]\n",
    "        mapping_dict[cluster] = set(indices)\n",
    "\n",
    "    nodes_df['cluster'] = nodes_df['cluster'].map(mapping_dict)\n",
    "    clusters = nodes_df['cluster'].drop_duplicates().tolist()\n",
    "    # assign \"to be visited on a day off\" nodes to the cluster\n",
    "    closed_day_problems = nodes_df[nodes_df.apply(lambda row: row['cluster'].issubset(row['closed_days']), axis=1)][['node_id', 'closed_days', 'x', 'y']]\n",
    "    closed_day_problems = closed_day_problems[closed_day_problems['node_id'] != 0]\n",
    "    if verbose:\n",
    "            print(f'number of nodes to be reassigned: {len(closed_day_problems)}')\n",
    "    for sub_row in closed_day_problems.iterrows():\n",
    "        closed_on = sub_row[1]['closed_days']\n",
    "        # find possible clusters (i.e.: not all closed on days contained)\n",
    "        possible_clusters = [cluster for cluster in clusters if not cluster.issubset(closed_on)]\n",
    "        possible_nodes = nodes_df[nodes_df['cluster'].isin(possible_clusters)]['node_id']\n",
    "        possible_nodes = possible_nodes[possible_nodes != 0]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'closed_on: {closed_on}, possible_clusters: {possible_clusters}')\n",
    "        # remove 0 from row and col of time_matrix\n",
    "        time_matrix_sub = time_matrix.drop(0, axis=0).drop(0, axis=1)\n",
    "        closest_node = time_matrix_sub.loc[sub_row[1]['node_id'], possible_nodes].idxmin()\n",
    "        closest_node_cluster = nodes_df.loc[closest_node, 'cluster']\n",
    "        closest_node_cluster_name = nodes_df.loc[closest_node, 'cluster_name']\n",
    "\n",
    "        if verbose:\n",
    "            print(f'reassigning node {sub_row[1][\"node_id\"]} with coordinates ({sub_row[1][\"x\"]}, {sub_row[1][\"y\"]}) to cluster {closest_node_cluster} since node {closest_node} with coordinates ({nodes_df.loc[closest_node, \"x\"]}, {nodes_df.loc[closest_node, \"y\"]}) is the closest node in a possible cluster')\n",
    "        # update the cluster of the node with the set defining the new cluster preventing \"Must have equal len keys and value when setting with an iterable\"\n",
    "        nodes_df.at[sub_row[0], 'cluster'] = closest_node_cluster\n",
    "        nodes_df.at[sub_row[0], 'cluster_name'] = closest_node_cluster_name\n",
    "    \n",
    "    # assign nodes with fixed appointments always to the cluster that contains the fixed appointments visit day in cluster index\n",
    "    for index_2, row in nodes_df.iterrows():\n",
    "        appointment = row['weekday_fixed_appointment']\n",
    "        if (appointment not in row['cluster']) and not pd.isnull(appointment):\n",
    "            if verbose:\n",
    "                print(f'{row[\"node_id\"]} has a fixed appointment on {appointment} but is assigned to cluster {row[\"cluster\"]}')\n",
    "            # find the cluster that contains the appointment day\n",
    "            cluster_with_appointment = [cluster for cluster in clusters if appointment in cluster][0]\n",
    "            cluster_name = nodes_df.loc[nodes_df['cluster'] == cluster_with_appointment, 'cluster_name'].iloc[0]\n",
    "            if verbose:\n",
    "                print(f'Assigning node {row[\"node_id\"]} to cluster {cluster_with_appointment}')\n",
    "            nodes_df.at[index_2, 'cluster'] = cluster_with_appointment\n",
    "            nodes_df.at[index_2, 'cluster_name'] = cluster_name\n",
    "\n",
    "    nodes_df['adjusted_opening_hours'] = nodes_df.apply(lambda row: adjust_opening_hours(row, work_schedule), axis=1)\n",
    "    \n",
    "    # nodes_df.groupby('cluster_name') and then get list of node_ids\n",
    "    clusters_and_nodes = nodes_df.groupby('cluster_name')['node_id'].apply(list)\n",
    "\n",
    "    # add 0 to start of node_ids if it's not already in the list\n",
    "    clusters_and_nodes = clusters_and_nodes.apply(lambda x: [0] + x if 0 not in x else x)\n",
    "\n",
    "    dropped_nodes = None\n",
    "    route_lists = {}\n",
    "    for cluster, node_ids in clusters_and_nodes.items():\n",
    "        if verbose:\n",
    "            print(f'initial nodes: {node_ids}')\n",
    "        sub_nodes_df = nodes_df[nodes_df['node_id'].isin(node_ids)]\n",
    "        # find cluster set (e.g.: {4})\n",
    "        cluster_set = sub_nodes_df['cluster'].iloc[0]\n",
    "        # if dropped nodes append them to sub_nodes_df\n",
    "        if dropped_nodes:\n",
    "            dropped_nodes_df = nodes_df[nodes_df['node_id'].isin(dropped_nodes)]\n",
    "            sub_nodes_df = pd.concat([sub_nodes_df, dropped_nodes_df]) # should append the dropped nodes to the end of the sub_nodes_df\n",
    "        # adjust all name and cluster\n",
    "        sub_nodes_df.loc[:, 'cluster_name'] = cluster\n",
    "        sub_nodes_df.loc[:, 'cluster'] = sub_nodes_df.loc[:, 'cluster'].apply(lambda _: cluster_set)\n",
    "        sub_nodes_df['adjusted_opening_hours'] = sub_nodes_df.apply(lambda row: adjust_opening_hours(row, work_schedule), axis=1)\n",
    "\n",
    "        # remove and warn about rows that have a weekday_fixed_appointment that is not in the cluster\n",
    "        invalid_rows = sub_nodes_df[sub_nodes_df.apply(\n",
    "            lambda row: not pd.isna(row['weekday_fixed_appointment']) and row['weekday_fixed_appointment'] not in row['cluster'], axis=1)]\n",
    "        if not invalid_rows.empty:\n",
    "            # print(f\"WARNING: Nodes {invalid_rows['node_id'].tolist()} have fixed appointments that can not take place on the assigned days.\")\n",
    "            sub_nodes_df = sub_nodes_df[~sub_nodes_df.index.isin(invalid_rows.index)]\n",
    "        \n",
    "        # find route -> dropped nodes\n",
    "        if verbose:\n",
    "            print(f'nodes for finding route: {sub_nodes_df[\"node_id\"].tolist()}')\n",
    "        result, dropped_nodes = solve_vrp(time_matrix, sub_nodes_df, deep=False, verbose=verbose)\n",
    "        route_lists[cluster] = result[1] # route without travel\n",
    "        if verbose:\n",
    "            print(f'Route for {cluster}: {result[0]}')\n",
    "        if (len(dropped_nodes) > 0) and verbose:\n",
    "            priority_dropped = sub_nodes_df[sub_nodes_df['node_id'].isin(dropped_nodes)]['priority'].mean().round(2)\n",
    "            priority_kept = sub_nodes_df[~sub_nodes_df['node_id'].isin(dropped_nodes)]['priority'].mean().round(2)\n",
    "            if verbose:\n",
    "                print(f\"Dropped nodes with mean priority {priority_dropped} vs. kept nodes with mean priority {priority_kept}\")\n",
    "    if visual:\n",
    "        plot_all_cluster_routes(route_lists, nodes_df)\n",
    "\n",
    "    total_route_length = 0\n",
    "    for trip in route_lists:\n",
    "        total_route_length += route_lists[trip][-1][1] - route_lists[trip][0][1]\n",
    "    options_df.at[index, 'nodes_considered'] = list(nodes_df['node_id'][nodes_df['cluster'].isin(clusters)])\n",
    "    options_df.at[index, 'num_nodes_considered'] = len(options_df.at[index, 'nodes_considered'])\n",
    "    options_df.at[index, 'avg_prio_nodes_considered'] = nodes_df[nodes_df['node_id'].isin(options_df.at[index, 'nodes_considered'])]['priority'].mean().round(2)\n",
    "    options_df.at[index, 'num_nodes_dropped'] = len(dropped_nodes)  # Assuming you have this info\n",
    "    options_df.at[index, 'total_road_time'] = total_route_length\n",
    "\n",
    "options_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
