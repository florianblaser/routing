{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  create dummy data and calculate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 60\n",
    "margin = 5 # time margin for arrival before appointment and stay beyond\n",
    "working_hours = [8 * 60, 18 * 60] # 8am to 6pm\n",
    "percentage_of_appointments = 0\n",
    "span_cost_coefficient = 20000 # adjust\n",
    "slack = 20000 # adjust\n",
    "penalty_factor = 300000\n",
    "lunch_start = 11 * 60  # 12 PM in minutes\n",
    "lunch_end = 13 * 60  # 2 PM in minutes\n",
    "lunch_duration = 30\n",
    "min_work_days = 6 # minimum number of work days per location\n",
    "\n",
    "# week constraints\n",
    "max_days_off = 0\n",
    "days_off = {}\n",
    "no_overnight_stays = {}\n",
    "max_overnight_stays = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.routing import create_nodes_dataframe, custom_clustering, plot_refined_clusters, assign_weekdays_to_clusters, plot_ind_route, plot_all_cluster_routes, create_data_model, plot_all_nodes_with_angles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- !!! can there be more than one fixed appointment per node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df, time_matrix = create_nodes_dataframe(num_nodes=num_nodes, min_work_days=min_work_days, home_node_id=0, visiting_interval_min=10, visiting_interval_max=30, max_last_visit=20, frac_fixed_app=percentage_of_appointments)\n",
    "nodes_df['weekdays_fixed_appointments'] = nodes_df['fixed_appointment'].apply(lambda x: x[0] if isinstance(x, tuple) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check eventualities of no fixed appointments or closed days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No days off specified.\n"
     ]
    }
   ],
   "source": [
    "all_days = set(range(1, 8))\n",
    "# Calculate open days as sets from the dictionary keys\n",
    "nodes_df['open_days'] = nodes_df['opening_hours'].apply(lambda x: set(x.keys()))\n",
    "\n",
    "# Calculate closed days\n",
    "nodes_df['closed_days'] = nodes_df['open_days'].apply(lambda x: set(all_days - x))\n",
    "\n",
    "# Filter for VIP nodes based on priority\n",
    "VIP_nodes = nodes_df[nodes_df['priority'] > 0.8]\n",
    "\n",
    "# For each VIP node, identify specific closed days that match the days off\n",
    "if len(days_off) == 0:\n",
    "    print(\"No days off specified.\")\n",
    "\n",
    "else:\n",
    "    for index, row in VIP_nodes.iterrows():\n",
    "        closed_days_off = set(row['closed_days']) & days_off\n",
    "        if closed_days_off:\n",
    "            closed_days_off_str = ', '.join(map(str, sorted(closed_days_off)))\n",
    "            print(f\"Warning: VIP node {row['node_id']} is closed on day(s) {closed_days_off_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_fixed_appointments = nodes_df['weekdays_fixed_appointments'].dropna().unique()\n",
    "# if any(days_off in weekdays_fixed_appointments for days_off in days_off):\n",
    "#     raise ValueError(\"Fixed appointments are scheduled on days off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- !!! integrate logic for half days if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate valid lists as provided\n",
    "def find_lists(current_list, current_sum, max_length, target_sum):\n",
    "    if current_sum > target_sum or len(current_list) > max_length:\n",
    "        return []\n",
    "    if current_sum == target_sum and len(current_list) <= max_length:\n",
    "        return [current_list]\n",
    "    results = []\n",
    "    for i in range(0, 8):\n",
    "        adjusted_sum = current_sum + (i if i > 0 else 1)  # Adjust sum for 0s treated as 1\n",
    "        results.extend(find_lists(current_list + [i], adjusted_sum, max_length, target_sum))\n",
    "    return results\n",
    "\n",
    "# Generate valid lists\n",
    "valid_lists = find_lists([], 0, 7, 7)\n",
    "\n",
    "# Function to repeat values in the list according to their integer value\n",
    "def repeat_values(lst, overnight_trips=0):\n",
    "    repeated_list = []\n",
    "    for num in lst:\n",
    "        if overnight_trips == 1:\n",
    "            repeated_list.extend([num] * (num - 1) + [0] if num > 0 else [0])\n",
    "        else:\n",
    "            repeated_list.extend([num] * num if num > 0 else [0])\n",
    "    return repeated_list\n",
    "\n",
    "# Calculate days off for each list\n",
    "def calculate_days_off(lst):\n",
    "    repeated_lst = repeat_values(lst)\n",
    "    days_off = []\n",
    "    for i in range(min(len(repeated_lst), 7)):\n",
    "        if repeated_lst[i] == 0:\n",
    "            days_off.append(i + 1)  # Use 1-based indexing for days\n",
    "    return days_off\n",
    "\n",
    "# Calculate trip days for each list\n",
    "def calculate_overnight_trips(lst):\n",
    "    repeated_lst = repeat_values(lst, overnight_trips=1)  \n",
    "    trip_days = []\n",
    "    for i in range(min(len(repeated_lst), 7)):\n",
    "        if repeated_lst[i] > 1:\n",
    "            trip_days.append(i + 1)  # Use 1-based indexing for days\n",
    "    return trip_days\n",
    "\n",
    "def calculate_overnight_stays(lst):\n",
    "    overnight_stays = 0\n",
    "    for i in range(min(len(lst), 7)):\n",
    "        if lst[i] > 1:\n",
    "            overnight_stays += lst[i] - 1\n",
    "    return overnight_stays\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'gaps': valid_lists,\n",
    "    # 'Sum': [sum(lst) for lst in valid_lists],  # Calculating sum normally, 0s count as 0\n",
    "    # 'Length': [len(lst) for lst in valid_lists],\n",
    "    'n_overnight_trips': [calculate_overnight_stays(lst) for lst in valid_lists],\n",
    "    'overnight_days': [calculate_overnight_trips(lst) for lst in valid_lists],\n",
    "    'off_days': [calculate_days_off(lst) for lst in valid_lists],\n",
    "    'n_days_off': [lst.count(0) for lst in valid_lists], \n",
    "}\n",
    "\n",
    "options_df = pd.DataFrame(data)\n",
    "\n",
    "# Filter the DataFrame based on the constraints\n",
    "options_df = options_df[options_df['off_days'].apply(lambda x: all(day in x for day in days_off))]\n",
    "options_df = options_df[options_df['overnight_days'].apply(lambda x: not any(day in x for day in no_overnight_stays))]\n",
    "options_df = options_df[options_df['n_overnight_trips'].apply(lambda x: x <= max_overnight_stays)]\n",
    "options_df = options_df[options_df['n_days_off'].apply(lambda x: x <= max_days_off)]\n",
    "# add a column to df containing a dictionary counting the number of times an integer >1 appears in the list\n",
    "options_df['blocks'] = options_df['gaps'].apply(lambda x: Counter([item for item in x if item > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- do simple solution finding for the above combinations, find best solution give cost of overnight stay, improve best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_all_nodes_with_angles(nodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in rare cases the below will make trouble because there are two large gaps and a cluster is entirely contained within the second largest leading to size = nan\n",
    "- reintegrate distance matrix and replace dist_metric based on max dist with basis on variance within distances normalized by (?) total variance among nodes AND/OR use angle range as part of metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric(nodes_df, global_max_dist, node_ids, cluster_id, print_ind_metrics=False):\n",
    "    if len(node_ids) > 0:\n",
    "        filtered_nodes_df = nodes_df[nodes_df['node_id'].isin(node_ids)]\n",
    "\n",
    "        num_nodes_metric = len(filtered_nodes_df) / len(nodes_df)\n",
    "        \n",
    "        priority_metric = filtered_nodes_df['priority'].nlargest(int(0.5 * len(filtered_nodes_df))).mean()\n",
    "        priority_metric = priority_metric / float(cluster_id.split('_')[0])\n",
    "\n",
    "        max_dist_to_root = filtered_nodes_df['dist_to_home'].max()\n",
    "        dist_metric = max_dist_to_root / global_max_dist\n",
    "\n",
    "        # prevent any metric from being nan\n",
    "        if np.isnan(num_nodes_metric):\n",
    "            # print(f'Problems with num_nodes_metric for {cluster_id}')\n",
    "            num_nodes_metric = 0.5\n",
    "        if np.isnan(priority_metric):\n",
    "            # print(f'Problems with priority_metric for {cluster_id}')\n",
    "            priority_metric = 0.3\n",
    "        if np.isnan(dist_metric):\n",
    "            # print(f'Problems with dist_metric for {cluster_id}')\n",
    "            dist_metric = 0.5\n",
    "\n",
    "        metric = num_nodes_metric + dist_metric / 6\n",
    "\n",
    "        if print_ind_metrics:\n",
    "            print(f'Cluster: {cluster_id}')\n",
    "            print(f\"Number of nodes metric: {num_nodes_metric}\")\n",
    "            print(f\"Priority metric: {priority_metric}\")\n",
    "            print(f\"Distance metric: {dist_metric}\")\n",
    "            print(f\"Overall metric: {metric}\")\n",
    "    else:\n",
    "        metric = 0\n",
    "\n",
    "        if print_ind_metrics:\n",
    "            print('Found a cluster wihtout nodes')\n",
    "    \n",
    "    return metric\n",
    "\n",
    "def adjust_angles(clusters, nodes_df, angle_sizes, degree_adj, global_max_dist, cluster_sizes, total_span, verbose):\n",
    "    metrics = {}\n",
    "    for cluster_id, node_ids in clusters.items():\n",
    "        metrics[cluster_id] = calculate_metric(nodes_df, global_max_dist, node_ids, cluster_id)\n",
    "\n",
    "    total_metric = sum(metrics.values())\n",
    "    # total_days = sum of each key multiplied by the value in cluster_sizes\n",
    "    total_days = sum([key * value for key, value in cluster_sizes.items()])\n",
    "    base_metric = total_metric / total_days\n",
    "    \n",
    "    target_metrics = {}\n",
    "    for cluster, metric in metrics.items():\n",
    "        size = float(cluster.split('_')[0])\n",
    "        target_metrics[cluster] = base_metric * size\n",
    "\n",
    "    new_angle_sizes = angle_sizes.copy()  # Copy existing angle sizes to modify\n",
    "    \n",
    "    deviations = {}\n",
    "    for cluster_id, metric in metrics.items():\n",
    "        size = float(cluster_id.split('_')[0])\n",
    "        soll_metric = target_metrics[cluster_id]\n",
    "        deviation = metric - soll_metric\n",
    "        deviations[cluster_id] = deviation\n",
    "        new_angle_sizes[cluster_id] -= deviation * degree_adj\n",
    "\n",
    "    # Normalize the new angles to ensure they sum to total_span\n",
    "    total_new_angles = sum(new_angle_sizes.values())\n",
    "    scale_factor = total_span / total_new_angles\n",
    "    for cluster_id in new_angle_sizes:\n",
    "        new_angle_sizes[cluster_id] *= scale_factor\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Deviations, metrics, and new angle sizes:\")\n",
    "        for cluster_id in clusters:\n",
    "            print(f\"Cluster {cluster_id} with deviation {round(deviations[cluster_id], 2)}, \"\n",
    "                  f'and initial angle size {round(angle_sizes[cluster_id], 2)}° '\n",
    "                  f\"has new angle size {round(new_angle_sizes[cluster_id], 2)}°.\")\n",
    "\n",
    "    return new_angle_sizes\n",
    "\n",
    "def custom_clustering(nodes_df, cluster_sizes, precision, home_node_id=0, verbose=False, visual=False):\n",
    "    # remove the home node from the nodes_df\n",
    "    if nodes_df.index[0] == 0:\n",
    "        nodes_df_copy = nodes_df.drop(0).copy()\n",
    "    \n",
    "    clusters = {}\n",
    "    for size, count in cluster_sizes.items():\n",
    "        for i in range(count):\n",
    "            clusters[f'{size}_day_trip_{i}'] = []\n",
    "    \n",
    "    angles = sorted(nodes_df_copy['angle_to_home'])\n",
    "    diffs = [angles[i + 1] - angles[i] for i in range(len(angles) - 1)]\n",
    "    diffs.append(360 - angles[-1] + angles[0])\n",
    "    \n",
    "    max_gap = max(diffs)\n",
    "    gap_start = angles[diffs.index(max_gap)]\n",
    "    gap_end = angles[(diffs.index(max_gap) + 1) % len(angles)]\n",
    "\n",
    "    max_gap = max(diffs)\n",
    "    total_span = 360 - max_gap\n",
    "\n",
    "    if verbose == True:\n",
    "        print(f\"Largest gap spans from {gap_start}° to {gap_end}°, covering {max_gap}° leaving a total span of {total_span} for locations.\")\n",
    "\n",
    "    total_equivalent_degrees = sum(count * size for size, count in cluster_sizes.items())\n",
    "    base_degree = total_span / total_equivalent_degrees\n",
    "\n",
    "    angle_sizes = {}\n",
    "    for size, count in cluster_sizes.items():\n",
    "        # Calculate the angular size for each cluster of this size\n",
    "        cluster_angle_size = base_degree * size\n",
    "        for i in range(count):\n",
    "            cluster_id = f'{size}_day_trip_{i}'\n",
    "            angle_sizes[cluster_id] = cluster_angle_size\n",
    "    global_max_dist = nodes_df_copy['dist_to_home'].max()\n",
    "\n",
    "    cluster_start = gap_end\n",
    "    degree_adj = total_span / 7\n",
    "\n",
    "    # Initial assignment of nodes to clusters\n",
    "    for i in range(precision):\n",
    "        current_angle = cluster_start  # Reset the start angle for each precision iteration\n",
    "            \n",
    "        # add the home node to each cluster\n",
    "        for key in clusters.keys():\n",
    "            clusters[key] = [home_node_id]\n",
    "\n",
    "        # Assign nodes to clusters based on their angle to the home node\n",
    "        for cluster_id, size in angle_sizes.items():\n",
    "            start_angle = current_angle\n",
    "            # round up to the nearest integer\n",
    "            start_angle = int(start_angle)\n",
    "            end_angle = (current_angle + size) % 360\n",
    "            end_angle = int(np.ceil(end_angle))\n",
    "            # Ensuring all nodes are assigned, handling the wrap-around scenario more cleanly\n",
    "            if end_angle < start_angle:  # This handles the case where the segment wraps past 360 degrees\n",
    "                nodes_in_cluster = [index for index, row in nodes_df_copy.iterrows() if \n",
    "                                    (row['angle_to_home'] >= start_angle or row['angle_to_home'] < end_angle)]\n",
    "            else:  # No wrap-around, normal case\n",
    "                nodes_in_cluster = [index for index, row in nodes_df_copy.iterrows() if \n",
    "                                    (start_angle <= row['angle_to_home'] < end_angle)]\n",
    "\n",
    "            clusters[cluster_id] = [home_node_id] + nodes_in_cluster\n",
    "            current_angle = end_angle\n",
    "\n",
    "        if (i == 0 and verbose):\n",
    "            print(\"Initial clusters:\")\n",
    "            for key, value in clusters.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "\n",
    "        if i % 10 == 0 and visual:\n",
    "            plot_refined_clusters(clusters, nodes_df)\n",
    "        \n",
    "        angle_sizes = adjust_angles(clusters, nodes_df_copy, angle_sizes, degree_adj, global_max_dist, cluster_sizes, total_span, verbose)\n",
    "        degree_adj *= 0.95\n",
    "\n",
    "        sum_of_angles = sum(angle_sizes.values())\n",
    "        if verbose:\n",
    "            print(f\"Sum of angles: {sum_of_angles} vs. total span: {total_span}\")\n",
    "        \n",
    "    return clusters\n",
    "\n",
    "def fit_blocks(blocks, gaps, solution, solutions, index=0):\n",
    "    if index == len(gaps):  # Check if we've addressed all gaps\n",
    "        if not blocks:  # Ensure all blocks have been used\n",
    "            solutions.append(solution)\n",
    "        return\n",
    "    \n",
    "    # Attempt to fit each block into the current gap\n",
    "    for i, block in enumerate(blocks):\n",
    "        # ['1_day_trip_5', '1_day_trip_0', '1_day_trip_1', '1_day_trip_3', '1_day_trip_4', '1_day_trip_2']\n",
    "        block_size = int(block[0])  # Extract size from block identifier, e.g., \"2A\" -> 2\n",
    "        if block_size <= gaps[index]:  # Check if block can fit in the current gap\n",
    "            # Setup for recursion: remove the block and reduce the gap size\n",
    "            new_blocks = blocks[:i] + blocks[i+1:]  # Remove current block\n",
    "            new_solution = [lst[:] for lst in solution]  # Copy solution to modify\n",
    "            new_solution[index].append(block)  # Add block to current gap's solution\n",
    "            \n",
    "            # Reduce the gap by the size of the block\n",
    "            new_gaps = gaps[:]\n",
    "            new_gaps[index] -= block_size\n",
    "            \n",
    "            # Move to next gap if current gap is exactly filled, otherwise continue\n",
    "            if new_gaps[index] == 0:\n",
    "                fit_blocks(new_blocks, new_gaps, new_solution, solutions, index + 1)\n",
    "            else:\n",
    "                fit_blocks(new_blocks, new_gaps, new_solution, solutions, index)\n",
    "\n",
    "def count_fixed_appointments(weekly_schedule, nodes_df):\n",
    "    count = 0\n",
    "    for i, cluster_day in enumerate(weekly_schedule):\n",
    "        start_day = i + 1\n",
    "        if 'day_off' not in cluster_day:\n",
    "            duration = int(cluster_day.split('_')[0])\n",
    "            end_day = start_day + duration\n",
    "            for day in range(start_day, end_day + 1):\n",
    "                count += nodes_df[(nodes_df['cluster'] == cluster_day) & (nodes_df['weekdays_fixed_appointments'] == day)].shape[0]\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual = False\n",
    "verbose = False\n",
    "options_df = options_df.sort_values(by='n_overnight_trips')\n",
    "for index, row in options_df.iterrows():\n",
    "    blocks = row['blocks']\n",
    "    gaps = row['gaps']\n",
    "    clusters = custom_clustering(nodes_df, blocks, precision=50, verbose=False, visual=False)\n",
    "\n",
    "    if verbose:\n",
    "        for cluster, nodes in clusters.items():\n",
    "            print(f\"Cluster {cluster}:\")\n",
    "            print(f\"Average node priority: {round(nodes_df.loc[nodes_df['node_id'].isin(nodes), 'priority'].mean(), 2)}\")\n",
    "            print(f\"Average distance to home: {round(nodes_df.loc[nodes_df['node_id'].isin(nodes), 'dist_to_home'].mean(), 2)}\")\n",
    "            print(f\"Count of nodes: {len(nodes)}\")\n",
    "\n",
    "    if visual:\n",
    "        plot_refined_clusters(clusters, nodes_df)\n",
    "\n",
    "    node_to_cluster = {node: cluster for cluster, nodes in clusters.items() for node in nodes}\n",
    "    nodes_df['cluster'] = nodes_df['node_id'].map(node_to_cluster)\n",
    "\n",
    "    # store positions of 0s in gaps to remove and add them to the solution after fitting blocks\n",
    "    zero_positions = [i for i, gap in enumerate(gaps) if gap == 0]\n",
    "    gaps = [gap for gap in gaps if gap > 0]\n",
    "    \n",
    "    blocks = list(nodes_df['cluster'].unique())\n",
    "    solution = [[] for _ in gaps]  # Initialize solution structure for each gap\n",
    "    solutions = []\n",
    "    if verbose:\n",
    "        print(f'Blocks: {blocks}, Gaps: {gaps}')\n",
    "    fit_blocks(blocks, gaps, solution, solutions)\n",
    "\n",
    "    # Add 0s back to the solution as 'day-off' clusters\n",
    "    for solution in solutions:\n",
    "        for position in zero_positions:\n",
    "            solution.insert(position, ['1_day_off'])\n",
    "    solutions = [[item for sublist in outer_list for item in sublist] for outer_list in solutions]\n",
    "    repeated_list = []\n",
    "    for sublist in solutions:\n",
    "        new_sublist = []\n",
    "        for item in sublist:\n",
    "            count = int(item.split('_')[0])  # Extract the number of repetitions\n",
    "            new_sublist.extend([item] * count)  # Repeat the item\n",
    "        \n",
    "        repeated_list.append(new_sublist)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Found {len(repeated_list)} solutions: {repeated_list}')\n",
    "\n",
    "    best_count = -float('inf')\n",
    "    best_list = None\n",
    "    # for each list capturing the allocations of clusters to weekdays\n",
    "    for i, sublist in enumerate(repeated_list):\n",
    "        if verbose:\n",
    "            print(f'solution {i+1}/{len(repeated_list)}')\n",
    "        count = 0\n",
    "        # for each cluster in the list\n",
    "        for cluster in set(sublist):\n",
    "            if not 'day_off' in cluster:\n",
    "                # Access the indices of the cluster in the week\n",
    "                relevant_weekdays = [i + 1 for i, gap in enumerate(sublist) if gap == cluster]\n",
    "                \n",
    "                \n",
    "                # get a list of non-unique fixed appointments\n",
    "                fixed_appointment_days = list(nodes_df[nodes_df['cluster'] == cluster]['weekdays_fixed_appointments'].dropna())\n",
    "                \n",
    "                cluster_count_fixed = len([entry for entry in fixed_appointment_days if entry in relevant_weekdays])\n",
    "                initial_count = count\n",
    "                count += cluster_count_fixed\n",
    "\n",
    "                closed_days = list(nodes_df[nodes_df['cluster'] == cluster]['closed_days'].dropna())\n",
    "                flat_closed_days = []\n",
    "                for s in closed_days:\n",
    "                    flat_closed_days.extend(s)\n",
    "                \n",
    "                cluster_count_closed = len([entry for entry in flat_closed_days if entry in relevant_weekdays])\n",
    "                count -= cluster_count_closed\n",
    "                if verbose:\n",
    "                    print(f'Relevant weekdays: {relevant_weekdays}')\n",
    "                    print(f'Fixed appointment days: {fixed_appointment_days}')\n",
    "                    print(\"Closed on:\", flat_closed_days)\n",
    "                    print(f'added {cluster_count_fixed} and deducted {cluster_count_closed} from {initial_count}')\n",
    "\n",
    "        if count > best_count:\n",
    "            if verbose:\n",
    "                print(f'replacing {best_count} with {count}')\n",
    "            best_count = count\n",
    "            best_list = sublist\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Best solution had a count of {best_count}')\n",
    "\n",
    "    mapping_dict = {}\n",
    "    for i, cluster in enumerate(best_list):\n",
    "        indices = [j+1 for j, x in enumerate(best_list) if x == cluster]\n",
    "        mapping_dict[cluster] = set(indices)\n",
    "\n",
    "    nodes_df['cluster'] = nodes_df['cluster'].map(mapping_dict)\n",
    "\n",
    "    clusters = nodes_df['cluster'].drop_duplicates().tolist()\n",
    "    # assign \"to be visited on a day off\" nodes to the cluster\n",
    "    closed_day_problems = nodes_df[nodes_df.apply(lambda row: row['cluster'].issubset(row['closed_days']), axis=1)][['node_id', 'closed_days', 'x', 'y']]\n",
    "    closed_day_problems = closed_day_problems[closed_day_problems['node_id'] != 0]\n",
    "    if verbose:\n",
    "            print(f'number of nodes to be reassigned: {len(closed_day_problems)}')\n",
    "    for sub_row in closed_day_problems.iterrows():\n",
    "        closed_on = sub_row[1]['closed_days']\n",
    "        # find possible clusters (i.e.: not all closed on days contained)\n",
    "        possible_clusters = [cluster for cluster in clusters if not cluster.issubset(closed_on)]\n",
    "        possible_nodes = nodes_df[nodes_df['cluster'].isin(possible_clusters)]['node_id']\n",
    "        possible_nodes = possible_nodes[possible_nodes != 0]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'closed_on: {closed_on}, possible_clusters: {possible_clusters}')\n",
    "        # remove 0 from row and col of time_matrix\n",
    "        time_matrix_sub = time_matrix.drop(0, axis=0).drop(0, axis=1)\n",
    "        closest_node = time_matrix_sub.loc[sub_row[1]['node_id'], possible_nodes].idxmin()\n",
    "        closest_node_cluster = nodes_df.loc[closest_node, 'cluster']\n",
    "        if verbose:\n",
    "            print(f'reassigning node {sub_row[1][\"node_id\"]} with coordinates ({sub_row[1][\"x\"]}, {sub_row[1][\"y\"]}) to cluster {closest_node_cluster} since node {closest_node} with coordinates ({nodes_df.loc[closest_node, \"x\"]}, {nodes_df.loc[closest_node, \"y\"]}) is the closest node in a possible cluster')\n",
    "        # update the cluster of the node with the set defining the new cluster preventing \"Must have equal len keys and value when setting with an iterable\"\n",
    "        nodes_df.at[sub_row[0], 'cluster'] = closest_node_cluster\n",
    "    \n",
    "    # assign nodes with fixed appointments always to the cluster that contains the fixed appointments visit day in cluster index\n",
    "    for index, row in nodes_df.iterrows():\n",
    "        appointment = row['weekdays_fixed_appointments']\n",
    "        if (appointment not in row['cluster']) and not pd.isnull(appointment):\n",
    "            # print(f'{row[\"node_id\"]} has a fixed appointment on {appointment} but is assigned to cluster {row[\"cluster\"]}')\n",
    "            # find the cluster that contains the appointment day\n",
    "            cluster_with_appointment = [cluster for cluster in clusters if appointment in cluster]\n",
    "            # print(f'Assigning node {row[\"node_id\"]} to cluster {cluster_with_appointment}')\n",
    "            nodes_df.at[index, 'cluster'] = cluster_with_appointment[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Integrate lunch breaks for locations\n",
    "- deduct on_site_time from closing times\n",
    "- double check that in all cases opening hours are kept for all relevant days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_minutes(t):\n",
    "    return t.hour * 60 + t.minute + t.second / 60\n",
    "\n",
    "def adjust_opening_hours(row):\n",
    "    clusters = row['cluster']\n",
    "    first_day = min(clusters)\n",
    "    opening_hours = row['opening_hours']\n",
    "    fixed_appointment = row['fixed_appointment']\n",
    "    adjusted_hours = {}\n",
    "    if fixed_appointment:\n",
    "        day, app_start, app_end = fixed_appointment\n",
    "        print(f'inital fixed appointment for node {row[\"node_id\"]} on day {day} from {app_start} to {app_end} in trip with first day {first_day}')\n",
    "        adjusted_open = time_to_minutes(app_start) + 1440 * (day - first_day) - margin \n",
    "        adjusted_close = time_to_minutes(app_end) + 1440 * (day - first_day) + margin\n",
    "        print(f'adjusting fixed appointment for node {row[\"node_id\"]} on day {day} from {app_start} to {app_end} to {adjusted_open} to {adjusted_close}')\n",
    "    else:\n",
    "        for day in clusters:\n",
    "            if day in opening_hours:\n",
    "                open_time, close_time = opening_hours[day]\n",
    "                adjusted_open = time_to_minutes(open_time)\n",
    "                adjusted_close = time_to_minutes(close_time)\n",
    "                if len(clusters) > 1:\n",
    "                    adjusted_open += 1440 * (day - first_day)\n",
    "                    adjusted_close += 1440 * (day - first_day)\n",
    "                adjusted_hours[int(day)] = (adjusted_open, adjusted_close)\n",
    "                \n",
    "    return adjusted_hours\n",
    "\n",
    "nodes_df['adjusted_opening_hours'] = nodes_df.apply(adjust_opening_hours, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring sets in the cluster column into a format that allows grouping\n",
    "nodes_df['cluster'] = nodes_df['cluster'].apply(lambda x: frozenset(x))\n",
    "refined_clusters = nodes_df.groupby('cluster')['node_id'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "depot_node_data = nodes_df[nodes_df['node_id'] == 0].iloc[0]  # Assuming there is always a row for node 0 in the original DataFrame\n",
    "\n",
    "result_dfs = {}\n",
    "for index, group in nodes_df.groupby('cluster'):\n",
    "    # Check if depot node is in the current group\n",
    "    if 0 not in group['node_id'].values:\n",
    "        # Append depot node data to the group\n",
    "        group = pd.concat([pd.DataFrame([depot_node_data]), group], ignore_index=True)\n",
    "    # Now group is guaranteed to include the depot node\n",
    "    result_dfs[index] = group[['node_id', 'priority', 'adjusted_opening_hours', 'cluster', 'on_site_time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nodes with fixed appointments should have prio 1\n",
    "- adapt for trips > 2\n",
    "- correct index for printing dropped nodes\n",
    "- optimize route beyond priority choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m         time_dimension\u001b[38;5;241m.\u001b[39mCumulVar(index)\u001b[38;5;241m.\u001b[39mRemoveInterval(working_hours[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1440\u001b[39m \u001b[38;5;241m*\u001b[39m (day\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), working_hours[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1440\u001b[39m \u001b[38;5;241m*\u001b[39m day)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mwindows\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1440\u001b[39m:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m# print('work on day 2')\u001b[39;00m\n\u001b[1;32m    119\u001b[0m         work_start \u001b[38;5;241m=\u001b[39m working_hours[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1440\u001b[39m\n\u001b[1;32m    120\u001b[0m         work_end \u001b[38;5;241m=\u001b[39m working_hours[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1440\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "sub_nodes_df = result_dfs[list(result_dfs.keys())[0]]\n",
    "solutions = {}\n",
    "route_lists = {}\n",
    "route_lists_with_travel = {}\n",
    "\n",
    "def minutes_to_hhmm(minutes_am):\n",
    "    minutes_am = minutes_am % 1440  # Ensure minutes are within a day\n",
    "    minutes = minutes_am % 60\n",
    "    hours = (minutes_am - minutes) // 60  # Use integer division for hours\n",
    "    return f'{hours:02}:{minutes:02}'\n",
    "\n",
    "def create_data_model(sub_nodes_df, sub_time_matrix):\n",
    "    \"\"\"Stores the data for the problem.\"\"\"\n",
    "    data = {}\n",
    "    data['time_matrix'] = sub_time_matrix\n",
    "    data['windows'] = sub_nodes_df['adjusted_opening_hours'].tolist()\n",
    "    data['priorities'] = sub_nodes_df['priority'].tolist()\n",
    "    data['num_vehicles'] = 1\n",
    "    data['on_site_time'] = sub_nodes_df['on_site_time'].tolist()\n",
    "    data['depot'] = 0\n",
    "    return data\n",
    "\n",
    "def return_route_and_times(solution, manager, routing, original_node_ids, data):\n",
    "    \"\"\"Returns the route along with the start times at each node.\"\"\"\n",
    "    index = routing.Start(0)  # Start at the depot.\n",
    "    route_with_travel = []\n",
    "    route_without_travel = []\n",
    "    time_dimension = routing.GetDimensionOrDie('total_time')  # Make sure this matches the dimension name used\n",
    "\n",
    "    while not routing.IsEnd(index):\n",
    "        node_index = manager.IndexToNode(index)\n",
    "        original_node_id = original_node_ids[node_index]  # Map back to original node ID\n",
    "        time_var = time_dimension.CumulVar(index)\n",
    "        start_time = solution.Min(time_var)\n",
    "        end_time = start_time + data['on_site_time'][node_index]  # Include on-site time\n",
    "        route_with_travel.append((original_node_id, start_time, end_time))  # Include end time for better clarity\n",
    "        route_without_travel.append((original_node_id, start_time))  # Include end time for better clarity\n",
    "        next_index = solution.Value(routing.NextVar(index))\n",
    "        \n",
    "        travel_time = routing.GetArcCostForVehicle(index, next_index, 0) - data['on_site_time'][index]  # Get travel time\n",
    "        route_with_travel.append((\"road\", travel_time))\n",
    "        \n",
    "        index = next_index\n",
    "\n",
    "    # Add the final node\n",
    "    final_node_index = manager.IndexToNode(index)\n",
    "    final_node_id = original_node_ids[final_node_index]\n",
    "    final_time_var = time_dimension.CumulVar(index)\n",
    "    final_start_time = solution.Min(final_time_var)\n",
    "    final_end_time = final_start_time + data['on_site_time'][final_node_index]\n",
    "    route_with_travel.append((final_node_id, final_start_time, final_end_time))\n",
    "    route_without_travel.append((final_node_id, final_start_time))\n",
    "\n",
    "    return route_with_travel, route_without_travel\n",
    "\n",
    "def print_route(route_with_times):\n",
    "    \"\"\"Prints the route in the desired format.\"\"\"\n",
    "    route_str = \"\"\n",
    "    for segment in route_with_times:\n",
    "        if segment[0] == \"road\":\n",
    "            route_str += f\" - road ({segment[1]}) - \"\n",
    "        else:\n",
    "            node_id, start_time, end_time = segment\n",
    "            route_str += f\"{node_id} ({minutes_to_hhmm(start_time)}-{minutes_to_hhmm(end_time)})\"\n",
    "    print(route_str)\n",
    "\n",
    "# def solve_vrp(key, sub_nodes_df):\n",
    "if True:\n",
    "    max_travel_time = 10000 if len(sub_nodes_df['cluster'].iloc[0]) == 1 else 20000 # adjust\n",
    "    nodes = sub_nodes_df['node_id'].tolist()\n",
    "    sub_time_matrix = time_matrix.loc[nodes, nodes].values.tolist()\n",
    "    sub_time_matrix = [[int(x) for x in row] for row in sub_time_matrix]\n",
    "    data = create_data_model(sub_nodes_df, sub_time_matrix)\n",
    "    manager = pywrapcp.RoutingIndexManager(len(data[\"time_matrix\"]), data[\"num_vehicles\"], data[\"depot\"])\n",
    "    routing = pywrapcp.RoutingModel(manager)\n",
    "    def time_callback(from_index, to_index):\n",
    "        from_node = manager.IndexToNode(from_index)\n",
    "        to_node = manager.IndexToNode(to_index)\n",
    "        return data[\"time_matrix\"][from_node][to_node] + data['on_site_time'][from_node]\n",
    "    transit_callback_index = routing.RegisterTransitCallback(time_callback)\n",
    "    routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n",
    "    routing.AddDimension(\n",
    "        transit_callback_index,\n",
    "        slack,  # upper bound for slack / waiting time\n",
    "        max_travel_time,  # upper bound for vehicle maximum travel time\n",
    "        False,  # start cumul to zero\n",
    "        \"total_time\"\n",
    "    )\n",
    "    time_dimension = routing.GetDimensionOrDie(\"total_time\")\n",
    "    time_dimension.SetGlobalSpanCostCoefficient(span_cost_coefficient)\n",
    "\n",
    "    # PENALTY\n",
    "    for location_index, priority in enumerate(data['priorities']):\n",
    "        index = manager.NodeToIndex(location_index)\n",
    "        if index == 0:\n",
    "            continue\n",
    "        else:\n",
    "            routing.AddDisjunction([index], int(round((priority*100)**2*penalty_factor, 0)))\n",
    "\n",
    "    # OPENING HOURS, LUNCH AND OVERNIGHT BREAKS\n",
    "    for location_index, windows in enumerate(data['windows']):\n",
    "        index = manager.NodeToIndex(location_index)\n",
    "        days = len(windows)\n",
    "        if days > 1:\n",
    "            if index < manager.GetNumberOfNodes():\n",
    "                work_start = working_hours[0]\n",
    "                work_end = working_hours[1] + 1440\n",
    "                latest_start = max(work_start, windows[0][0])\n",
    "                earliest_end = min(work_end, windows[-1][1])\n",
    "                # print(f'setting the time window from {latest_start} to {earliest_end} for node {location_index}')\n",
    "                time_dimension.CumulVar(index).SetRange(latest_start, earliest_end)\n",
    "            for day in range(1, days):\n",
    "                # print(f'and removing time between days from {working_hours[1] + 1440 * (day-1)} to {working_hours[0] + 1440 * day} for node {location_index}')\n",
    "                time_dimension.CumulVar(index).RemoveInterval(windows[day-1][1], windows[day][0])\n",
    "                time_dimension.CumulVar(index).RemoveInterval(working_hours[1] + 1440 * (day-1), working_hours[0] + 1440 * day)\n",
    "        else:\n",
    "            if windows[0][0] > 1440:\n",
    "                # print('work on day 2')\n",
    "                work_start = working_hours[0] + 1440\n",
    "                work_end = working_hours[1] + 1440\n",
    "                # print(f'work starts at {work_start} and ends at {work_end}')\n",
    "                # print(f'window starts at {windows[0][0]} and ends at {windows[0][1]}')\n",
    "                day_start = max(windows[0][0], work_start)\n",
    "                day_end = min(windows[0][1], work_end)\n",
    "            else:\n",
    "                day_start = windows[0][0]\n",
    "                day_end = windows[0][1]\n",
    "            # print(f'setting the time window from {day_start} to {day_end} for node {location_index}')\n",
    "            time_dimension.CumulVar(index).SetRange(day_start, day_end)  \n",
    "\n",
    "    node_visit_transit = {}\n",
    "    for index in range(routing.Size()):\n",
    "        node = manager.IndexToNode(index)\n",
    "        node_visit_transit[index] = data['on_site_time'][node]\n",
    "\n",
    "    lunch_break_interval = routing.solver().FixedDurationIntervalVar(\n",
    "        lunch_start, lunch_end, lunch_duration, False, 'lunch_break'\n",
    "    )\n",
    "\n",
    "    # Assign the break interval to the single vehicle\n",
    "    time_dimension.SetBreakIntervalsOfVehicle([lunch_break_interval], 0, node_visit_transit)\n",
    "\n",
    "    # Instantiate route start and end times to produce feasible times\n",
    "    routing.AddVariableMinimizedByFinalizer(time_dimension.CumulVar(routing.Start(0)))\n",
    "    routing.AddVariableMinimizedByFinalizer(time_dimension.CumulVar(routing.End(0)))\n",
    "\n",
    "    # Setting first solution heuristic\n",
    "    search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "    search_parameters.first_solution_strategy = (\n",
    "        routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n",
    "    # search_parameters.local_search_metaheuristic = (\n",
    "    #     routing_enums_pb2.LocalSearchMetaheuristic.SIMULATED_ANNEALING)\n",
    "    search_parameters.time_limit.seconds = 300\n",
    "    search_parameters.log_search = False\n",
    "\n",
    "    # Solve the problem\n",
    "    solution = routing.SolveWithParameters(search_parameters)\n",
    "\n",
    "#     if solution:\n",
    "#         dropped = []\n",
    "#         for node in range(routing.Size()):\n",
    "#             if routing.IsStart(node) or routing.IsEnd(node):\n",
    "#                 continue\n",
    "#             if solution.Value(routing.NextVar(node)) == node:\n",
    "#                 dropped.append(manager.IndexToNode(node))\n",
    "#         if len(dropped) > 0:\n",
    "#             print(f\"dropped from {key}: {dropped}\")\n",
    "#         return key, return_route_and_times(solution, manager, routing, nodes, data)\n",
    "        \n",
    "#     else:\n",
    "#         print(f\"No solution for key {key}\")\n",
    "#         return key, None\n",
    "    \n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     future_to_key = {executor.submit(solve_vrp, key, sub_nodes_df): key for key, sub_nodes_df in result_dfs.items()}\n",
    "#     for future in concurrent.futures.as_completed(future_to_key):\n",
    "#         key = future_to_key[future]\n",
    "#         # print(f\"Finding solution for key: {key}\")\n",
    "#         try:\n",
    "#             key, result = future.result()\n",
    "#             if result:\n",
    "#                 route_lists_with_travel[key] = result[0]\n",
    "#                 route_lists[key] = result[1]\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error for key {key}: {e}\")\n",
    "\n",
    "# for key, route in route_lists_with_travel.items():\n",
    "#     print(f\"Route for key {key}:\")\n",
    "#     print_route(route)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[555], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m route_and_times \u001b[38;5;241m=\u001b[39m route_lists[\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroute_lists\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m      2\u001b[0m route_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(route_and_times, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrival_time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(nodes_df, route_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madjusted_opening_hours\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrival_time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "route_and_times = route_lists[list(route_lists.keys())[0]]\n",
    "route_df = pd.DataFrame(route_and_times, columns=['node_id', 'arrival_time'])\n",
    "merged_df = pd.merge(nodes_df, route_df, on='node_id', how='left')[['adjusted_opening_hours', 'arrival_time', 'node_id']]\n",
    "merged_df = merged_df[merged_df['node_id'] != 0]\n",
    "\n",
    "def check_within_ranges(arrival_time, ranges):\n",
    "    if len(ranges) == 1:\n",
    "        return ranges[0][0] <= arrival_time <= ranges[0][1]\n",
    "    elif len(ranges) == 2:\n",
    "        return (ranges[0][0] <= arrival_time <= ranges[0][1]) or (ranges[1][0] <= arrival_time <= ranges[1][1])\n",
    "    return False\n",
    "\n",
    "merged_df['time_check'] = merged_df.apply(\n",
    "    lambda row: check_within_ranges(row['arrival_time'], row['adjusted_opening_hours']), axis=1\n",
    ")\n",
    "\n",
    "if merged_df['time_check'].all():\n",
    "    print('VIOLATION OF TIME CONSTRAINTS')\n",
    "    print(merged_df[merged_df['time_check'] == False])\n",
    "\n",
    "# merged_df[['node_id', 'adjusted_opening_hours', 'arrival_time', 'time_check']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- print more info (opening hours, fixed appointments, priority, decisions [removing a node, replacing a node to another day, priority, etc])\n",
    "- use data to fine tune hyperparameters\n",
    "\n",
    "evtl:\n",
    "- Store state and if it was possible to find routes for all solutions iteratively add nodes based on node priority and distance to root node (those further away should be included more likely)\n",
    "- Test Discrete Priority (must be visited this week; must be visited next week; ...)\n",
    "- Compare routes with and without overnight stays / large clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_refined_clusters(refined_clusters, nodes_df, home_node_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_cluster_routes(route_lists, nodes_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
